---
title: '\huge Application of GARCH frameworks in dynamic volatility modeling and forecasting'
# <!-- author: | -->
# <!--   Erich Forst   -->
# <!--   _WHU – Otto Beisheim School of Management_   -->
# <!--   Vallendar, Germany   -->
# <!--   and   -->
# <!--   Ben Litzinger   -->
# <!--   _WHU – Otto Beisheim School of Management_   -->
# <!--   Vallendar, Germany -->
# date: '1st December 2024'
fontsize: 11pt
header-includes:
  - \usepackage{setspace}
  - \setstretch{1.25}
  - \usepackage{changepage}
output: 
  pdf_document:
    # toc: true
  html_document:
    # toc: true
    # number_sections: true
    # toc_float: true
    theme: cerulean
---

\thispagestyle{empty}
\Large   
\begin{center}
Erich Forst  

\textit{WHU – Otto Beisheim School of Management} 

Vallendar, Germany  
  
and  
  
Ben Litzinger  

\textit{WHU – Otto Beisheim School of Management}  

Vallendar, Germany

\vspace{0.5cm}

1st December 2024

\end{center}
\setstretch{1.5}
\small
\vspace{1cm}
\begin{adjustwidth}{1cm}{1cm}
\begin{center}
\textbf{\large Abstract}
\end{center}
This assignment applies logarithmic returns derived from stock closing prices to model percentage changes and  facilitate volatility analysis. Tests for ARCH effects confirm the presence of autoregressive conditional heteroscedasticity, consistent with volatility clustering. A GARCH(2,2) model is estimated, with parameter stability evaluated under alternative optimization algorithms to assess the impact of solver choice. This model is also used to forecast the financial time series volatility. A Monte Carlo simulation examines the properties of long-run variance estimates derived from a simulated GARCH(1,1) process across repeated iterations. The analysis demonstrates the practical application of GARCH models for understanding and forecasting conditional volatility in financial markets.


\end{adjustwidth}
\setstretch{1.25}
\newpage

\tableofcontents

\newpage

# 1 Preparing stock price data for Eaton Corporation (ETN)

## 1.1 Downloading and visualizing historical stock closing prices

The historical stock price data for Eaton Corporation (ETN) is downloaded from Yahoo Finance for the period January 1, 2000 to November 11, 2024. Non-trading days, such as weekends and holidays, are automatically excluded from the data, eliminating the need for additional filtering. Yahoo Finance data is returned as an "xts" object by default. However, the code includes an additional check to ensure the data is explicitly in "xts" format. The plot of ETN’s closing prices provides a visual summary of the stock’s historical performance.

\vspace{0.3cm}
\footnotesize
\setstretch{0.5}

```{r 1.1_download_data, include=TRUE, message=FALSE, fig.dim=c(8, 4)}
library(quantmod)

#specify of stock ticker symbol
ticker <- "ETN"

#download of yahoo finance data
getSymbols(ticker,
           from="2000-01-01",
           to="2024-11-12",
           src="yahoo")

#conversion into xts object
if (!inherits(get(ticker), "xts")) {
  assign(ticker, as.xts(get(ticker)))
}
```


```{r 1.1_plot_stock, include=TRUE, message=FALSE, fig.dim=c(8, 4)}
#plot of the stock price development
plot(ETN$ETN.Close,
     main="ETN stock closing prices",
     ylab="stock price in USD",
     col="darkblue",
     main.timespan=FALSE,
     format.labels="%Y",
     yaxis.right=FALSE,
     type="h")
```

\small
\setstretch{1.25}

## 1.2 Transforming stock closing prices into log return series

Log returns of Eaton Corporation’s stock, denoted as $R_t$, are computed to approximate the daily percentage changes in its closing prices:
\begin{equation}
R_t = \ln(P_t) - \ln(P_{t-1}) = \ln\left(\frac{P_t}{P_{t-1}}\right)
\end{equation}

where $P_t$ represents the closing price at time $t$, and $P_{t-1}$ is the closing price at the previous time period. This transformation expresses the relative change between consecutive closing prices on a logarithmic scale.

The resulting series includes a missing value for the first observation since the calculation requires a lagged price ($P_{t-1}$) that is unavailable for the initial time step. This missing value is removed to have no NAs in the dataset. By visualizing the log returns, the daily percentage changes in Eaton Corporation’s stock prices highlight periods of high and low volatility.

\vspace{0.3cm}
\footnotesize
\setstretch{0.5}

```{r 1.2_log_returns, include=TRUE, message=FALSE, fig.dim=c(8, 4)}
#transform closing prices into log returns
ETN$log_returns <- diff.xts(log(ETN$ETN.Close),
                            lag=1,
                            differences=1)

#omit NAs
ETN <- na.omit(ETN)

#plot log returns of closing price
plot(ETN$log_returns,
     main="Daily percentage changes of ETN",
     main.timespan=FALSE,
     ylab="Log returns",
     col="darkblue",
     lwd=0.5,
     format.labels="%Y",
     yaxis.right=FALSE)
```

\small
\setstretch{1.25}

## 1.3 Analyzing normality in log returns visually

The following analysis visually evaluates whether ETN’s log returns approximate a normal distribution. The empirical density is represented by the blue histogram, while the red curve depicts the theoretical normal density calculated using the sample mean and standard deviation. This comparison provides an intuitive, preliminary check for normality but does not constitute a formal statistical test.

The histogram of ETN's log returns closely resembles a normal distribution; however, the density around the mean is slightly higher than expected under a true normal distribution, indicating a minor deviation from normality.

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 1.3_normality_analysis, echo=FALSE, fig.dim=c(8, 4)}
mean_log_returns <- mean(ETN$log_returns,
                         na.rm=TRUE)

sd_log_returns <- sd(ETN$log_returns,
                     na.rm=TRUE)

x_vals <- seq(min(ETN$log_returns, na.rm=TRUE),
              max(ETN$log_returns, na.rm=TRUE),
              length.out=1000)

y_vals <- dnorm(x_vals,
                mean=mean_log_returns,
                sd=sd_log_returns)

hist(ETN$log_returns,
     freq=FALSE,
     breaks=100,
     xlab="log returns",
     main="Density of log returns against normal distribution",
     col="darkblue",
     xlim=c(-0.1,0.1))

lines(x_vals,
      y_vals,
      col="red",
      lwd=2)
```

\small
\setstretch{1.25}

# 2 Analyzing autocorrelation and ARCH effects

The analysis evaluates autocorrelation in log returns and squared log returns to identify temporal dependencies and volatility clustering. Autocorrelation quantifies the relationship between current values and past values of a time series at a given lag. 
 
## 2.1  Assessing temporal dependencies and volatility clustering

For instance, the autocorrelation at lag $j$ is defined as:

\begin{equation} 
\rho_j = \frac{\text{Cov}(R_t, R_{t-j})}{\sqrt{\text{Var}(R_t) \cdot \text{Var}(R_{t-j})}} \end{equation}

where $\text{Cov}(R_t, R_{t-j})$ is the covariance between log returns at time $t$ and $t-j$, and $\text{Var}(R_t)$ and $\text{Var}(R_{t-j})$ represent their variances.

The autocorrelograms reveal distinct patterns: log returns show almost no autocorrelation, indicating they are uncorrelated and unpredictable over time. In contrast, squared log returns exhibit some autocorrelation, reflecting volatility clustering. This phenomenon suggests periods of high or low volatility tend to persist.

\vspace{0.3cm}
\footnotesize
\setstretch{0.5}

```{r 2.1_autocorrelogram,include=TRUE, fig.dim=c(8, 6)}
#use forecast library to create autocorrelograms
library(forecast)


par(mfrow=c(2,2))
#plot autocorrelation of log-returns
acf <- Acf(ETN$log_returns,
           lag.max=10,
           type="correlation",
           plot=TRUE,
           main="Autocorrelogram of returns",
           xlab="lag",
           ylab="correlation",
           ylim=c(-0.3, 1))

#square log_returns
ETN$sq_log_returns <- ETN$log_returns**2


#plot autocorrelation of squared log-returns
acf <- Acf(ETN$sq_log_returns,
           lag.max=10,
           type="correlation",
           plot=TRUE,
           main="Autocorrelogram of squared returns",
           xlab="lag",
           ylab="correlation",
           ylim=c(-0.3, 1))

```

\small
\setstretch{1.25}

## 2.2 Testing for ARCH effects

To formally test for ARCH effects, the Ljung-Box test is conducted, which evaluates whether the residuals' squared terms (a proxy for variance) are correlated. The null hypothesis ($H_0$) assumes no autocorrelation in the squared residuals, which implies the absence of ARCH effects. The Box-Ljung test statistic is computed as:

\begin{equation}
LB=n \sum_{i=1}^{m} \frac{n+2}{n-i} r_i^2
\end{equation}

where $r_i^2$ is the squared autocorrelation at lag $i$, $n$ is the sample size, and $m$ is the number of lags.

This statistic follows a $\chi^2$ distribution with $m$ degrees of freedom.

\vspace{0.3cm}
\footnotesize
\setstretch{0.5}

```{r 2.2_Ljung-Box_test, include=TRUE}
#perform Box-Ljung to test for ARCH effects
ETN_meanadj <- lm(ETN$log_returns ~ 1)
ETN$meanadj_returns <- ETN_meanadj$residuals
ETN$uhat_sq <- ETN$meanadj_returns**2
Ljung_Box_Test <- Box.test(ETN$uhat_sq,
                             lag=10,
                             type="Ljung-Box")

print(Ljung_Box_Test)
```

\small
\setstretch{1.25}

In the analysis, the Box-Ljung test applied to squared residuals yields a p-value of $2.2 \times 10^{-16}$, which is substantially below common significance thresholds (e.g., 0.05). Therefore, $H_0$ is rejected, confirming that past residuals are significantly correlated with the current variance. This result aligns with the visual evidence from the autocorrelograms of squared returns, confirming the presence of volatility clustering and ARCH effects.

## 2.3 Testing for ARCH effects with the BIC

The following analysis evaluates the Bayesian Information Criterion (BIC) for autoregressive (AR) models of different lag orders, using the log returns of the financial time series data. By decomposing the BIC calculation into its constituent terms, we gain insights into the trade-off between model fit (via residual sum of squares) and complexity (penalizing additional parameters). The results reveal that the lowest BIC value occurs at lag 1, indicating that the AR(1) model offers the best balance between simplicity and explanatory power. Additionally, R² values were computed to assess the proportion of variance explained by each model.

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r, echo=FALSE, results='asis', message=FALSE}
library(stargazer)

bic_values <- numeric(10)
term1 <- numeric(10)
term2 <- numeric(10)
R2_values <- numeric(10)
lags <- 1:10
T_val <- length(na.omit(ETN$log_returns))

# Compute Total Sum of Squares (TSS)
log_returns_mean <- mean(ETN$log_returns, na.rm = TRUE)
TSS <- sum((ETN$log_returns - log_returns_mean)^2, na.rm = TRUE)  
  
# Fit AR models with different numbers of lags and calculate BIC
for (lag in lags) {
  ar_model <- arima(ETN$log_returns, order = c(lag, 0, 0))  # AR model with 'lag' lags
  bic_values[lag] <- BIC(ar_model)

  residuals <- ar_model$residuals
  RSS <- sum(residuals^2)
  # Computing the BIC
  term1[lag] <- log(RSS / T_val)
  term2[lag] <- (lag + 1) * log(T_val) / T_val
  bic_values[lag] <- log(RSS / T_val) + (lag + 1) * (log(T_val) / T_val)
  
  # Compute R2
  R2_values[lag] <- 1 - (RSS / TSS)
}

bic_table <- data.frame(Lag=lags,
                        term1=term1,
                        term2=term2,
                        BIC=bic_values,
                        R2=R2_values)

# Create a stargazer table
stargazer(bic_table,
          title = "BIC Values for Different Lags",
          summary = FALSE, 
          rownames = FALSE,
          header = FALSE,
          digits = 5,
          type="latex")
```

\vspace{0.3cm} 
\small
\setstretch{1.25}
\newpage

# 3 Specification and Estimation of a GARCH(2,2) Model

## 3.1 Comparing optimisation routines for GARCH estimation

To estimate a GARCH(2,2) model, the rugarch library is employed, which facilitates the specification and estimation. The model is designed to capture conditional heteroscedasticity by modeling the conditional variance (\(\sigma_t^2\)) as a function of two lagged squared residuals (\(\epsilon_{t-1}^2, \epsilon_{t-2}^2\)) and two lagged conditional variances (\(\sigma_{t-1}^2, \sigma_{t-2}^2\)). 

The mean model assumes no autoregressive or moving average components (ARMA(0,0)) and includes only a constant term. This is expressed as:

\begin{equation}
R_t = \mu + \epsilon_t,
\end{equation}

where \(R_t\) is the observed time series (e.g., log returns), \(\mu\) is the constant mean, and \(\epsilon_t\) is the error term, which follows a conditional variance structure.

The GARCH(2,2) model is defined by the following equation for the conditional variance:

\begin{equation}
\sigma_t^2 = \omega + \alpha_1 \epsilon_{t-1}^2 + \alpha_2 \epsilon_{t-2}^2 + \beta_1 \sigma_{t-1}^2 + \beta_2 \sigma_{t-2}^2,
\end{equation}

where \(\sigma_t^2\) is the conditional variance at time \(t\), \(\omega\) is the constant term, \(\alpha_1\) and \(\alpha_2\) are the coefficients for lagged squared residuals, and \(\beta_1\) and \(\beta_2\) are the coefficients for lagged conditional variances.

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 3.1_garch22_comparison, include=TRUE, message=FALSE, warning=TRUE}
#load library rugarch to use GARCH models
library(rugarch)

#specify garch(2,2)
garch22_spec <- ugarchspec(variance.model=list(garchOrder=c(2,2)),
                           mean.model=list(armaOrder=c(0,0),include.mean=TRUE),
                           distribution.model="norm")

#estimate the model with solnp
garch22_fit_solnp <- ugarchfit(garch22_spec,
                             ETN$log_returns,
                             solver="solnp")

# round(garch22_fit_solnp@fit[["matcoef"]],
#        digits=8)

#estimate the model with lbfgs
garch22_fit_lbfgs <- ugarchfit(garch22_spec,
                              ETN$log_returns,
                              solver="lbfgs",
                              solver.control=list(pgtol=0.3, maxit = 10e5))

# round(garch22_fit_lbfgs@fit[["matcoef"]],
#        digits=8)
```

\vspace{0.3cm} 
\small
\setstretch{1.25}

## 3.2 Interpreting the signifance of the estimators

After estimating the GARCH(2,2) model using both the $\textit{solnp}$ and $\textit{lbfgs}$ optimization routine, the significance of the estimated parameters can be judged. Notably, the coefficient $\alpha_1$, which represents the impact of past trading day's shocks (or innovations), is highly significant with a $p$-value close to zero for both optimisation routines. Surprisingly, the second lagged residual term, $\alpha_2$, is entirely insignificant ($p = 1$), indicating that shocks from two days ago have no measurable effect on the conditional variance.

The parameter $\beta_1$ which represents the persistence of volatility from the previous period is found to be statistically significant for $\textit{lbfgs}$ routine with a p-value of 0.001. However, in the $\textit{solnp}$ routine $\beta_1$ is not statistically significant with a p-value of 0.514.

Similarly, the coefficient $\beta_2$ which captures the effect of volatility from two periods ago, is significant for lbfgs with a p-value of 0.020, whereas for solnp, it is not significant, with a p-value of 
0.606. 

The estimation of the constant is also significant for $\textit{lbfgs (p=0)}$ but not for $\textit{solnp (p=0.179)}$.

The inconsistent results for estimating $\beta_0$ and $\beta_0$ may be attributed to the specific solver control settings. The lbfgs routine optimization was performed with the settings $\textit{solver.control=list(pgtol=0.3, maxit=10e5)}$ which allowed for a larger number of iterations and a higher tolerance for convergence.

A larger pgtol value (like 0.3) means the optimization process can stop earlier, even if the gradient isn't close to zero. This can lead to faster convergence, but at the risk of stopping before the true optimal solution is found. 0.3 was the smallest possible value that allowed the optimization to converge.

The value for maxit sets the maximum number of iterations before stopping the optimization process, ensuring the solver has enough opportunities to find the best-fitting parameters.

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 3.2_stargazer, echo=FALSE, results='asis', message=FALSE}
library(stargazer)
garch22_coef_lbfgs <- round(garch22_fit_lbfgs@fit[["matcoef"]], digits = 8)
garch22_coef_solnp <- round(garch22_fit_solnp@fit[["matcoef"]], digits = 8)

# extracting the p-values
pvals_lbfgs <- garch22_fit_lbfgs@fit[["matcoef"]][, 4]
pvals_solnp <- garch22_fit_solnp@fit[["matcoef"]][, 4]

# function, to display small values as 0
format_p_values <- function(pvals) {
  pvals[pvals < 0.0001] <- 0
  return(round(pvals, 3))
}

# formatting p-values
formatted_lbfgs_p_values <- format_p_values(pvals_lbfgs)
formatted_solnp_p_values <- format_p_values(pvals_solnp)

garch22_coef_df <- data.frame(
  Parameter = rownames(garch22_coef_lbfgs), 
  lbfgs_estimate = garch22_coef_lbfgs[, 1],
  lbfgs_p_value = formatted_lbfgs_p_values,
  solnp_estimate = garch22_coef_solnp[, 1],
  solnp_p_value = formatted_solnp_p_values
)

garch22_coef_df_filtered <- garch22_coef_df[-1,]

stargazer(garch22_coef_df_filtered,
          summary = FALSE, 
          rownames = FALSE,
          header = FALSE,
          title = "Comparison of GARCH(2,2) Parameter Estimates from 'lbfgs' and 'solnp' Solvers")
```


\vspace{0.3cm} 
\small
\setstretch{1.25}
\newpage

# 4 Plotting and forecasting the conditional standard deviation

## 4.1 Plotting mean adjusted returns with superimposed standard deviations

To analyze the dynamics of time-varying volatility in financial returns, the mean-adjusted returns are plotted alongside the estimated conditional standard deviation $\pm \hat{\sigma}_t$, derived from a GARCH(2,2) model. 

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 4.1_mean_adj_ret_plot,include=TRUE, fig.dim=c(8, 4)}
#extract positive and negative estimated sigma from GARCH fit
ETN$sig_t_hat_garch22 <- sigma(garch22_fit_solnp)
ETN$neg_sig_t_hat_garch22 <- -ETN$sig_t_hat_garch22

#plot mean-adjusted returns with superimposed +/- estimated sigmas
plot(ETN[, c('sig_t_hat_garch22', 'neg_sig_t_hat_garch22', 'meanadj_returns')],
     ylab=expression(epsilon[t] ~ "and" ~ "\u00B1" ~ widehat(sigma)[t]),
     lwd=c(0.5, 0.5, 0.25),
     col=c('red', 'red', 'blue'),
     format.labels="%Y",
     main="Mean adjusted returns and estimated conditional standard deviation",
     main.timespan=FALSE,
     yaxis.right=FALSE,
     )
```

\vspace{0.3cm} 
\small
\setstretch{1.25}

## 4.2 Forecasting volatility with a GARCH(2,2) model

Next, a one-step-ahead forecast is computed, which provides the forecasted volatility, $\hat{\sigma}_{t+1}$.

The equation for the conditional variance in the GARCH(2,2) model is:


\begin{equation}
\hat{\sigma}_{t+1}^2=\omega + \alpha_1 \epsilon_t^2 + \alpha_2 \epsilon_{t-1}^2 + \beta_1 \sigma_t^2 + \beta_2 \sigma_{t-1}^2
\end{equation}


\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 4.2_garch_forecast, include=TRUE}
#forecast next day with GARCH(2,2) fit
garch22_fcast <- ugarchforecast(garch22_fit_solnp, n.ahead=1)
fcast_val <- as.numeric(garch22_fcast@forecast$sigmaFor)

#input the forecasted value into the existing matrix
last_date <- tail(index(ETN), 1)
next_date <- last_date + 1

ETN$fcast_val <- NA

new_row <- xts(matrix(NA, ncol=ncol(ETN)), order.by=next_date)
colnames(new_row) <- colnames(ETN)
new_row[1, "fcast_val"] <- fcast_val

ETN <- rbind(ETN, new_row)
```

\vspace{0.3cm} 
\small
\setstretch{1.25}

# 5 Simulating and estimating a GARCH(1,1) model

## 5.1 Simulating Monte Carlo Observations for Analyzing GARCH Model Properties

In this analysis, a GARCH(1,1) model is estimated and simulated to evaluate the properties of parameter estimates and the unconditional variance. 

Using the predefined parameter values $\omega=0.1$, $\alpha_1=0.199999$, and $\beta_1=0.8$, 5000 observations are simulated from the GARCH model. The simulated data is fitted onto a GARCH(1,1) model with unknown parameters. The resulting parameter estimates for $\omega$, $\alpha_1$, and $\beta_1$ are extracted.

The unconditional variance is calculated for each simulation run to assess the stationarity condition of the model:

\begin{equation}
\text{Var}(R_t)=\frac{\omega}{1 - \alpha_1 - \beta_1}
\end{equation}

Intuitively, it captures the balance between the constant term ($\omega$) and the contributions of past shocks ($\alpha_1$) and past variances ($\beta_1$). For the model to be stationary, the condition $\alpha_1 + \beta_1 < 1$ must hold; otherwise, the denominator would approach zero or become negative, leading to an undefined or infinite variance.

This process is repeated 1,000 times, saving the estimations into a matrix.

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 5.1_Monte_Carlo, include=TRUE, message=FALSE}
#define parameters
n <- 5000
M <- 1000

mu <- 0
omega <- 0.1
alpha1 <- 0.199999
beta1 <- 0.8

#initialize matrix to save values
estimates <- matrix(ncol=4, nrow=M)
colnames(estimates) <- c("omega", "alpha1", "beta1", "uncon_var")

#specify GARCH(1,1) with predefined parameters
garch11_fixed_pars_spec <- ugarchspec(variance.model=list(garchOrder=c(1,1)),
                               mean.model=list(armaOrder=c(0,0), include.mean=TRUE),
                               fixed.pars=list(mu=mu, omega=omega, alpha1=alpha1, beta1=beta1),
                               distribution.model="norm")

#specify GARCH(1,1) without predefined parameters
garch11_spec <- ugarchspec(variance.model=list(garchOrder=c(1,1)),
                           mean.model=list(armaOrder=c(0,0), include.mean=TRUE),
                           distribution.model="norm")

for(i in 1:M){
  #simulate 5000 observations of specified GARCH
  garch11_sim=ugarchpath(garch11_fixed_pars_spec, n.sim=n)

  #fit the simulated on values onto a GARCH(1,1)
  garch11_fit <- ugarchfit(garch11_spec, garch11_sim@path[["seriesSim"]])

  estimates[i, "omega"] <- garch11_fit@fit$robust.matcoef["omega", 1]
  estimates[i, "alpha1"] <- garch11_fit@fit$robust.matcoef["alpha1", 1]
  estimates[i, "beta1"] <- garch11_fit@fit$robust.matcoef["beta1", 1]
  estimates[i, "uncon_var"] <- estimates[i, "omega"]/(1-estimates[i, "alpha1"]-estimates[i, "beta1"])
}
```

\vspace{0.3cm} 
\small
\setstretch{1.25}

## 5.2 Visualizing the parameters and the uncondtional variance

The parameter estimates ($\omega$, $\alpha_1$, $\beta_1$) and the unconditional variance are visualized through histograms to understand their distribution across multiple simulation runs. These visualizations provide insights into the accuracy of parameter estimation and the stationarity condition under the assumed model.

\vspace{0.3cm} 
\footnotesize
\setstretch{0.5}

```{r 5.2_Monte_Carlo_hist, include=TRUE}
par(mfrow=c(2,2))
hist(estimates[,1], breaks=50, freq=FALSE, main="omega (0.1)", col="blue")
hist(estimates[,2], breaks=50, freq=FALSE, main="alpha1 (0.199999)", col="blue")
hist(estimates[,3], breaks=50, freq=FALSE, main="beta1 (0.8)", col="blue")
hist(estimates[,4], breaks=50, freq=FALSE, main="volatility", col="blue")
```

\vspace{0.3cm} 
\small
\setstretch{1.25}

## 5.3 Interpreting the estimators

Using the true parameter values $\omega = 0.1$, $\alpha_1 = 0.199999$, and $\beta_1 = 0.8$, the unconditional variance of the GARCH(1,1) process is calculated as:

\begin{equation}
\frac{0.1}{1 - 0.199999 - 0.8} = \frac{0.1}{0.000001} = 100,000.
\end{equation}

The true unconditional variance is large due to the model's proximity to the stationarity boundary, where $1 - \alpha_1 - \beta_1$ approaches zero. The simulation, however, reveals a bimodal distribution of the estimated unconditional variance. 

The first peak of the distribution, around 10, reflects cases where the estimated parameters deviate substantially from the true values, leading to a larger denominator and a smaller unconditional variance value. The second peak, near 110, corresponds to simulations where the estimated parameters are closer to their true values, producing a statistic closer to the theoretical value. The trough between 50 and 70 represents a transition zone in the parameter space that occurs less frequently.